"use client";

import { useState, useRef, useCallback, useEffect } from "react";

export type ConnectionState = "disconnected" | "connecting" | "connected" | "error";
export type ConversationState = "idle" | "listening" | "thinking" | "speaking";

interface UseGeminiLiveOptions {
  mcId: string;
  theme: string;
  memo?: string;
  onMessage?: (text: string, isUser: boolean) => void;
  onStateChange?: (state: ConversationState) => void;
  onChapterChange?: (chapter: number, name: string, label: string) => void;
  onQuoteExtracted?: (quote: string) => void;
  onAutoEnd?: () => void; // MCãŒç· ã‚ã®è¨€è‘‰ã‚’è¨€ã£ãŸã‚‰è‡ªå‹•çµ‚äº†
  onError?: (error: string) => void;
}

interface Message {
  role: "user" | "assistant";
  content: string;
  timestamp: number;
}

// ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ï¼ˆ**...**å½¢å¼ï¼‰ã‚„è‹±èªã®å†…éƒ¨ãƒ¡ãƒ¢ã‚’é™¤å»
function cleanTranscript(text: string | object): string {
  // ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®å ´åˆã¯æ–‡å­—åˆ—ã«å¤‰æ›ã‚’è©¦ã¿ã‚‹
  if (typeof text === "object" && text !== null) {
    // textãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ãŒã‚ã‚Œã°ãã‚Œã‚’ä½¿ç”¨
    const obj = text as { text?: string };
    if (obj.text && typeof obj.text === "string") {
      text = obj.text;
    } else {
      console.log("âš ï¸ äºˆæœŸã—ãªã„ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆå½¢å¼:", JSON.stringify(text).substring(0, 100));
      return "";
    }
  }
  
  if (!text || typeof text !== "string") return "";
  
  // **...** å½¢å¼ã®æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ã‚’é™¤å»
  let cleaned = text.replace(/\*\*[^*]+\*\*/g, "");
  
  // æ—¥æœ¬èªæ–‡å­—ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
  const hasJapanese = /[\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FAF]/.test(cleaned);
  
  // æ—¥æœ¬èªãŒå«ã¾ã‚Œã¦ã„ãªã„å ´åˆã¯ç©ºã‚’è¿”ã™ï¼ˆè‹±èªã®ã¿ã®ç™ºè¨€ã¯ç„¡è¦–ï¼‰
  if (!hasJapanese) {
    console.log("âš ï¸ æ—¥æœ¬èªãªã—ã®ç™ºè¨€ã‚’ã‚¹ã‚­ãƒƒãƒ—:", text.substring(0, 50) + "...");
    return "";
  }
  
  // å…ˆé ­ã®è‹±èªã®æ€è€ƒãƒ†ã‚­ã‚¹ãƒˆã‚’é™¤å»ï¼ˆæ—¥æœ¬èªãŒå§‹ã¾ã‚‹ã¾ã§ã‚¹ã‚­ãƒƒãƒ—ï¼‰
  const japaneseMatch = cleaned.match(/[\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FAF]/);
  if (japaneseMatch && japaneseMatch.index !== undefined && japaneseMatch.index > 0) {
    cleaned = cleaned.substring(japaneseMatch.index);
  }
  
  // ç©ºç™½ã‚’ãƒˆãƒªãƒ 
  cleaned = cleaned.trim();
  
  return cleaned;
}

export function useGeminiLive(options: UseGeminiLiveOptions) {
  const { mcId, theme, memo, onMessage, onStateChange, onChapterChange, onQuoteExtracted, onAutoEnd, onError } = options;

  const [connectionState, setConnectionState] = useState<ConnectionState>("disconnected");
  const [conversationState, setConversationState] = useState<ConversationState>("idle");
  const [messages, setMessages] = useState<Message[]>([]);
  const [isAudioEnabled, setIsAudioEnabled] = useState(true);

  // conversationStateã®Refï¼ˆã‚¯ãƒ­ãƒ¼ã‚¸ãƒ£ã§ã®å‚ç…§ç”¨ï¼‰
  const conversationStateRef = useRef<ConversationState>("idle");
  useEffect(() => {
    conversationStateRef.current = conversationState;
  }, [conversationState]);

  const wsRef = useRef<WebSocket | null>(null);
  const audioContextRef = useRef<AudioContext | null>(null);
  const mediaStreamRef = useRef<MediaStream | null>(null);
  const processorRef = useRef<ScriptProcessorNode | null>(null);
  const audioQueueRef = useRef<AudioBuffer[]>([]);
  const isPlayingRef = useRef(false);
  const reconnectTimeoutRef = useRef<NodeJS.Timeout | null>(null);
  const inactivityTimeoutRef = useRef<NodeJS.Timeout | null>(null);
  // Web Speech APIã¯å‰Šé™¤ - Gemini APIã®inputTranscriptionã‚’ä½¿ç”¨
  
  // ãƒ‡ã‚£ãƒ¬ã‚¯ã‚¿ãƒ¼æ©Ÿèƒ½ç”¨
  const messageCountRef = useRef(0);
  const lastDirectorCheckRef = useRef(0);
  const startTimeRef = useRef<number | null>(null); // ä¼šè©±é–‹å§‹æ™‚åˆ»
  const currentChapterRef = useRef(1); // ç¾åœ¨ã®ãƒãƒ£ãƒ—ã‚¿ãƒ¼ï¼ˆ1=èµ·, 2=æ‰¿, 3=è»¢, 4=çµï¼‰
  const DIRECTOR_CHECK_INTERVAL = 5; // 5ç™ºè¨€ã”ã¨ã«ãƒ‡ã‚£ãƒ¬ã‚¯ã‚¿ãƒ¼ã«ç¢ºèªï¼ˆé »åº¦ã‚’ä¸‹ã’ã‚‹ï¼‰
  
  // ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ç™ºè¨€ã‚’è“„ç©ã™ã‚‹ãƒãƒƒãƒ•ã‚¡
  const mcBufferRef = useRef<string>("");
  const userBufferRef = useRef<string>("");
  
  // å‰²ã‚Šè¾¼ã¿æ¤œå‡ºç”¨ï¼ˆãƒ‡ãƒã‚¦ãƒ³ã‚¹ï¼‰- ã•ã‚‰ã«ä¿å®ˆçš„ã«
  const speechDetectionCountRef = useRef(0);
  const lastInterruptTimeRef = useRef(0);
  const SPEECH_DETECTION_THRESHOLD = 0.25; // RMSé–¾å€¤ã‚’ã•ã‚‰ã«ä¸Šã’ã‚‹ï¼ˆ0.15â†’0.25ï¼‰
  const SPEECH_DETECTION_FRAMES = 20; // 20ãƒ•ãƒ¬ãƒ¼ãƒ é€£ç¶šã§æ¤œå‡ºã—ãŸã‚‰å‰²ã‚Šè¾¼ã¿ï¼ˆç´„1ç§’ï¼‰
  const INTERRUPT_COOLDOWN = 5000; // å‰²ã‚Šè¾¼ã¿å¾Œ5ç§’é–“ã¯å†å‰²ã‚Šè¾¼ã¿ã—ãªã„

  // çŠ¶æ…‹å¤‰æ›´ã‚’ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã«é€šçŸ¥
  useEffect(() => {
    onStateChange?.(conversationState);
  }, [conversationState, onStateChange]);

  // messagesã‚’Refã§ä¿æŒï¼ˆã‚¯ãƒ­ãƒ¼ã‚¸ãƒ£å•é¡Œå›é¿ï¼‰
  const messagesRef = useRef<Message[]>([]);
  useEffect(() => {
    messagesRef.current = messages;
  }, [messages]);

  // ãƒ‡ã‚£ãƒ¬ã‚¯ã‚¿ãƒ¼ã«æŒ‡ç¤ºã‚’æ±‚ã‚ã‚‹
  const checkDirector = useCallback(async () => {
    const currentMessages = messagesRef.current;
    console.log(`ğŸ¬ ãƒ‡ã‚£ãƒ¬ã‚¯ã‚¿ãƒ¼ãƒã‚§ãƒƒã‚¯: ${currentMessages.length}ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸`);
    
    if (currentMessages.length < 4) {
      console.log("   â†’ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ•°ä¸è¶³ã§ã‚¹ã‚­ãƒƒãƒ—");
      return;
    }
    
    try {
      console.log("   â†’ APIå‘¼ã³å‡ºã—ä¸­...");
      const response = await fetch("/api/director", {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({
          conversationHistory: currentMessages,
          theme,
          memo,
          mcId,
          currentChapter: currentChapterRef.current,
        }),
      });

      if (!response.ok) {
        console.error("   â†’ API ã‚¨ãƒ©ãƒ¼:", response.status);
        return;
      }

      const data = await response.json();
      
      // ãƒãƒ£ãƒ—ã‚¿ãƒ¼é€²è¡Œã®å‡¦ç†ï¼ˆæœ€å¤§4ãƒãƒ£ãƒ—ã‚¿ãƒ¼ã¾ã§ï¼‰
      const MAX_CHAPTER = 4;
      if (data.shouldAdvanceChapter && data.chapterInfo && currentChapterRef.current < MAX_CHAPTER) {
        const newChapter = currentChapterRef.current + 1;
        if (newChapter <= MAX_CHAPTER) {
          currentChapterRef.current = newChapter;
          console.log(`ğŸ¬ ãƒãƒ£ãƒ—ã‚¿ãƒ¼é€²è¡Œ: ${data.chapterInfo.name}ã€Œ${data.chapterInfo.label}ã€`);
          console.log(`   ç†ç”±: ${data.advanceReason}`);
          onChapterChange?.(newChapter, data.chapterInfo.name, data.chapterInfo.label);
        }
      }
      
      // MCã¸ã®æŒ‡ç¤ºé€ä¿¡ï¼ˆç©æ¥µçš„ã«ä»‹å…¥ã™ã‚‹ï¼‰
      if (data.instruction && wsRef.current?.readyState === WebSocket.OPEN) {
        console.log("ğŸ“‹ ãƒ‡ã‚£ãƒ¬ã‚¯ã‚¿ãƒ¼æŒ‡ç¤º:", data.instruction);
        if (data.groundingTip) {
          console.log("ğŸ’¡ ãƒã‚¿æ´»ç”¨:", data.groundingTip);
        }
        
        // æŒ‡ç¤ºã‚’MCã«é€ä¿¡ï¼ˆä¼šè©±ã®æµã‚Œã‚’æ”¹å–„ã™ã‚‹ãŸã‚ç©æ¥µçš„ã«ä»‹å…¥ï¼‰
        let instructionText = `[ãƒ‡ã‚£ãƒ¬ã‚¯ã‚¿ãƒ¼ã‹ã‚‰ã®æŒ‡ç¤º] ${data.instruction}`;
        
        if (data.shouldAdvanceChapter && data.chapterInfo) {
          instructionText += `\n[ãƒãƒ£ãƒ—ã‚¿ãƒ¼ç§»è¡Œ] ${data.chapterInfo.name}ã€Œ${data.chapterInfo.label}ã€ã«é€²ã‚“ã§ãã ã•ã„ã€‚`;
        }
        
        if (data.groundingTip) {
          instructionText += `\n[ãƒã‚¿æƒ…å ±] ${data.groundingTip}`;
        }
        
        // MCãŒè©±ã—ã¦ã„ãªã„æ™‚ã€ã¾ãŸã¯ä¼šè©±ãŒæ­¢ã¾ã£ã¦ã„ã‚‹æ™‚ã«é€ä¿¡
        // ä¼šè©±ã‚’ä¸­æ–­ã—ãªã„ã‚ˆã†ã€MCã®æ¬¡ã®ã‚¿ãƒ¼ãƒ³ã§åæ˜ ã•ã‚Œã‚‹ã‚ˆã†ã«é€ä¿¡
        const directorMessage = {
          clientContent: {
            turns: [
              {
                role: "user",
                parts: [{ text: instructionText }],
              },
            ],
            turnComplete: true,
          },
        };
        
        // æŒ‡ç¤ºã‚’é€ä¿¡ï¼ˆMCã®æ¬¡ã®ç™ºè©±æ™‚ã«åæ˜ ã•ã‚Œã‚‹ï¼‰
        wsRef.current.send(JSON.stringify(directorMessage));
        console.log("ğŸ“¤ ãƒ‡ã‚£ãƒ¬ã‚¯ã‚¿ãƒ¼æŒ‡ç¤ºã‚’MCã«é€ä¿¡ã—ã¾ã—ãŸ");
      }
      
      // å¼•ç”¨æŠ½å‡ºï¼šè¨˜äº‹ã«ä½¿ãˆãã†ãªãƒ•ãƒ¬ãƒ¼ã‚ºã‚’ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
      if (data.notableQuote) {
        console.log("ğŸ’¬ ãƒ”ãƒƒã‚¯ã‚¢ãƒƒãƒ—:", data.notableQuote);
        onQuoteExtracted?.(data.notableQuote);
      }
    } catch (error) {
      console.error("Director check failed:", error);
    }
  }, [theme, memo, mcId, onChapterChange, onQuoteExtracted]);

  // éã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆ5åˆ†ï¼‰
  const resetInactivityTimeout = useCallback(() => {
    if (inactivityTimeoutRef.current) {
      clearTimeout(inactivityTimeoutRef.current);
    }
    inactivityTimeoutRef.current = setTimeout(() => {
      console.log("â° éã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ - åˆ‡æ–­");
      disconnect();
    }, 5 * 60 * 1000); // 5åˆ†
  }, []);

  // éŸ³å£°å†ç”Ÿã®ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°ç”¨
  const nextPlayTimeRef = useRef<number>(0);
  const activeSourcesRef = useRef<AudioBufferSourceNode[]>([]);

  // éŸ³å£°å†ç”Ÿã‚­ãƒ¥ãƒ¼ã®å‡¦ç†ï¼ˆã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°æ–¹å¼ã§ãƒ–ãƒ„åˆ‡ã‚Œã‚’è§£æ¶ˆï¼‰
  const playNextAudio = useCallback(async () => {
    if (audioQueueRef.current.length === 0) return;
    if (!audioContextRef.current) return;

    isPlayingRef.current = true;
    const buffer = audioQueueRef.current.shift()!;
    const ctx = audioContextRef.current;

    const source = ctx.createBufferSource();
    source.buffer = buffer;
    source.connect(ctx.destination);

    // ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°: å‰ã®éŸ³å£°ãŒçµ‚ã‚ã‚‹æ™‚é–“ã‹ã‚‰é–‹å§‹
    const startTime = Math.max(ctx.currentTime, nextPlayTimeRef.current);
    nextPlayTimeRef.current = startTime + buffer.duration;

    source.onended = () => {
      // ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã‚½ãƒ¼ã‚¹ã‹ã‚‰å‰Šé™¤
      activeSourcesRef.current = activeSourcesRef.current.filter(s => s !== source);
      
      if (audioQueueRef.current.length === 0 && activeSourcesRef.current.length === 0) {
        isPlayingRef.current = false;
        setConversationState("listening");
      }
    };

    activeSourcesRef.current.push(source);
    source.start(startTime);

    // ã‚­ãƒ¥ãƒ¼ã«æ®‹ã‚ŠãŒã‚ã‚Œã°ç¶šã‘ã¦å‡¦ç†
    if (audioQueueRef.current.length > 0) {
      playNextAudio();
    }
  }, []);

  // å‰²ã‚Šè¾¼ã¿å‡¦ç† - å†ç”Ÿä¸­ã®éŸ³å£°ã‚’å³åº§ã«åœæ­¢
  const interruptPlayback = useCallback(() => {
    audioQueueRef.current = [];
    isPlayingRef.current = false;
    nextPlayTimeRef.current = 0;
    
    // ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªéŸ³å£°ã‚½ãƒ¼ã‚¹ã‚’å…¨ã¦åœæ­¢
    activeSourcesRef.current.forEach(source => {
      try {
        source.stop();
      } catch {
        // æ—¢ã«åœæ­¢ã—ã¦ã„ã‚‹å ´åˆã¯ç„¡è¦–
      }
    });
    activeSourcesRef.current = [];
  }, []);

  // Base64 PCM16éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’AudioBufferã«å¤‰æ›
  const decodeAudioData = useCallback(async (base64Audio: string): Promise<AudioBuffer | null> => {
    if (!audioContextRef.current) return null;

    try {
      // Base64ãƒ‡ã‚³ãƒ¼ãƒ‰
      const binaryString = atob(base64Audio);
      const bytes = new Uint8Array(binaryString.length);
      for (let i = 0; i < binaryString.length; i++) {
        bytes[i] = binaryString.charCodeAt(i);
      }

      // PCM16 (Int16) ã‚’ Float32 ã«å¤‰æ›
      const int16Data = new Int16Array(bytes.buffer);
      const float32Data = new Float32Array(int16Data.length);
      for (let i = 0; i < int16Data.length; i++) {
        float32Data[i] = int16Data[i] / 32768.0;
      }

      // AudioBuffer ã‚’æ‰‹å‹•ã§ä½œæˆ
      const audioBuffer = audioContextRef.current.createBuffer(1, float32Data.length, 24000);
      audioBuffer.getChannelData(0).set(float32Data);
      
      return audioBuffer;
    } catch (error) {
      console.error("Failed to decode audio:", error);
      return null;
    }
  }, []);

  // WebSocketæ¥ç¶š
  const connect = useCallback(async () => {
    if (connectionState === "connected" || connectionState === "connecting") return;

    setConnectionState("connecting");

    try {
      // ç¾åœ¨ã®ä¼šè©±å±¥æ­´ã‚’å–å¾—ï¼ˆå†æ¥ç¶šåˆ¤å®šç”¨ï¼‰
      const currentMessages = messagesRef.current;
      const isReconnecting = currentMessages.length > 0;

      // ã‚µãƒ¼ãƒãƒ¼ã‹ã‚‰è¨­å®šã‚’å–å¾—
      const configResponse = await fetch("/api/gemini-live", {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({ mcId, theme, memo, action: "get_config" }),
      });

      if (!configResponse.ok) {
        throw new Error("Failed to get Gemini Live config");
      }

      const config = await configResponse.json();

      // AudioContextåˆæœŸåŒ–ï¼ˆå‡ºåŠ›ç”¨ - Geminiã¯24kHzã§è¿”ã™ï¼‰
      audioContextRef.current = new AudioContext({ sampleRate: 24000 });

      // ãƒã‚¤ã‚¯ã‚¢ã‚¯ã‚»ã‚¹ï¼ˆã‚¨ã‚³ãƒ¼ã‚­ãƒ£ãƒ³ã‚»ãƒ«æœ‰åŠ¹ï¼‰
      mediaStreamRef.current = await navigator.mediaDevices.getUserMedia({
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true,
          sampleRate: 16000,
        },
      });

      // Gemini Live API WebSocketæ¥ç¶š
      const wsUrl = `wss://generativelanguage.googleapis.com/ws/google.ai.generativelanguage.v1beta.GenerativeService.BidiGenerateContent?key=${config.apiKey}`;
      console.log("WebSocketæ¥ç¶šé–‹å§‹...", config.model);
      
      const ws = new WebSocket(wsUrl);
      wsRef.current = ws;

      ws.onopen = () => {
        console.log("WebSocketæ¥ç¶šå®Œäº†");

        // ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é€ä¿¡
        const setupMessage = {
          setup: {
            model: `models/${config.model}`,
            generationConfig: {
              responseModalities: ["AUDIO"],
              speechConfig: {
                voiceConfig: {
                  prebuiltVoiceConfig: {
                    voiceName: config.voiceName || "Aoede", // MCã«åˆã‚ã›ãŸå£°
                  },
                },
                languageCode: "ja-JP",
              },
            },
            // ãƒˆãƒ©ãƒ³ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³è¨­å®šã‚’æœ‰åŠ¹åŒ–
            inputAudioTranscription: {},
            outputAudioTranscription: {},
            systemInstruction: {
              parts: [{ text: config.systemPrompt }],
            },
          },
        };

        ws.send(JSON.stringify(setupMessage));
        console.log("ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å®Œäº†å¾…æ©Ÿä¸­...");
      };

      ws.onmessage = async (event) => {
        resetInactivityTimeout();

        let data;
        try {
          if (event.data instanceof Blob) {
            const text = await event.data.text();
            data = JSON.parse(text);
          } else {
            data = JSON.parse(event.data);
          }
        } catch (parseError) {
          console.error("Failed to parse message:", parseError, event.data);
          return;
        }

        // ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å®Œäº†
        if (data.setupComplete) {
          const currentMessages = messagesRef.current;
          const isReconnecting = currentMessages.length > 0;

          if (isReconnecting) {
            console.log("âœ… å†æ¥ç¶šå®Œäº† - ä¼šè©±ã‚’å†é–‹ã—ã¾ã™ï¼ˆä¼šè©±å±¥æ­´:", currentMessages.length, "ä»¶ï¼‰");
          } else {
            console.log("âœ… æ¥ç¶šå®Œäº† - ç•ªçµ„é–‹å§‹");
            startTimeRef.current = Date.now(); // ä¼šè©±é–‹å§‹æ™‚åˆ»ã‚’è¨˜éŒ²
          }

          setConnectionState("connected");
          setConversationState("speaking");

          // å†æ¥ç¶šæ™‚ã¯ä¼šè©±å±¥æ­´ã‚’é€ä¿¡ã€åˆå›æ¥ç¶šæ™‚ã¯åˆæœŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é€ä¿¡
          if (isReconnecting) {
            // ä¼šè©±å±¥æ­´ã‚’Gemini Live APIã®å½¢å¼ã«å¤‰æ›
            const historyTurns = currentMessages.map((msg) => ({
              role: msg.role === "user" ? "user" : "model",
              parts: [{ text: msg.content }],
            }));

            // ä¼šè©±å±¥æ­´ã‚’é€ä¿¡ã—ã¦ä¼šè©±ã‚’å†é–‹
            const resumeMessage = {
              clientContent: {
                turns: historyTurns,
                turnComplete: true,
              },
            };
            ws.send(JSON.stringify(resumeMessage));
            console.log("ğŸ“ ä¼šè©±å±¥æ­´ã‚’é€ä¿¡ã—ã¾ã—ãŸï¼ˆ", historyTurns.length, "ä»¶ï¼‰");
          } else {
            // åˆæœŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é€ä¿¡ã—ã¦ä¼šè©±ã‚’é–‹å§‹
            const startMessage = {
              clientContent: {
                turns: [
                  {
                    role: "user",
                    parts: [{ text: "æ—¥æœ¬èªã§ç•ªçµ„ã‚’é–‹å§‹ã—ã¦ãã ã•ã„ã€‚ãƒªã‚¹ãƒŠãƒ¼ã«æŒ¨æ‹¶ã—ã¦ã€ã‚²ã‚¹ãƒˆã‚’ç´¹ä»‹ã—ã¦ãã ã•ã„ã€‚" }],
                  },
                ],
                turnComplete: true,
              },
            };
            ws.send(JSON.stringify(startMessage));
          }

          // ãƒã‚¤ã‚¯å…¥åŠ›ã®å‡¦ç†ã‚’é–‹å§‹
          startAudioCapture();
        }

        // ã‚µãƒ¼ãƒãƒ¼ã‹ã‚‰ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
        if (data.serverContent) {
          const content = data.serverContent;
          
          // ãƒ‡ãƒãƒƒã‚°: æœªå‡¦ç†ã®serverContentã‚­ãƒ¼ã‚’ç¢ºèª
          const keys = Object.keys(content);
          const handledKeys = ["modelTurn", "inputTranscription", "outputTranscription", "turnComplete", "generationComplete"];
          const unhandledKeys = keys.filter(k => !handledKeys.includes(k));
          if (unhandledKeys.length > 0) {
            console.log("ğŸ“¨ æœªå‡¦ç†ã‚­ãƒ¼:", unhandledKeys);
          }

          // ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®éŸ³å£°ãƒˆãƒ©ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆinputTranscriptionï¼‰- ãƒãƒƒãƒ•ã‚¡ã«è“„ç©
          if (content.inputTranscription) {
            const text = typeof content.inputTranscription === 'string' 
              ? content.inputTranscription 
              : (content.inputTranscription as { text?: string })?.text || '';
            if (text.trim()) {
              userBufferRef.current += text;
            }
          }

          // MCã®éŸ³å£°ãƒˆãƒ©ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆoutputTranscriptionï¼‰- ãƒãƒƒãƒ•ã‚¡ã«è“„ç©
          if (content.outputTranscription) {
            const cleanedText = cleanTranscript(content.outputTranscription);
            if (cleanedText) {
              mcBufferRef.current += cleanedText;
            }
          }

          // ãƒ†ã‚­ã‚¹ãƒˆå¿œç­”ï¼ˆmodelTurn.partsï¼‰
          // â€»outputTranscriptionã§ãƒãƒƒãƒ•ã‚¡ãƒªãƒ³ã‚°ã—ã¦ã„ã‚‹ãŸã‚ã€ã“ã“ã§ã¯ãƒ†ã‚­ã‚¹ãƒˆã¯ç„¡è¦–
          // éŸ³å£°å¿œç­”ã®ã¿å‡¦ç†
          if (content.modelTurn?.parts) {
            for (const part of content.modelTurn.parts) {
              // éŸ³å£°å¿œç­”
              if (part.inlineData?.mimeType?.startsWith("audio/") && isAudioEnabled) {
                setConversationState("speaking");
                const audioBuffer = await decodeAudioData(part.inlineData.data);
                if (audioBuffer) {
                  audioQueueRef.current.push(audioBuffer);
                  playNextAudio();
                }
              }
            }
          }

          // ã‚¿ãƒ¼ãƒ³å®Œäº† - ãƒãƒƒãƒ•ã‚¡ã‚’ãƒ•ãƒ©ãƒƒã‚·ãƒ¥ã—ã¦ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¨˜éŒ²
          if (content.turnComplete) {
            // MCã®ç™ºè¨€ã‚’ã¾ã¨ã‚ã¦è¨˜éŒ²
            if (mcBufferRef.current.trim()) {
              const fullText = mcBufferRef.current.trim();
              console.log("ğŸ™ï¸ MC:", fullText);
              const msg: Message = {
                role: "assistant",
                content: fullText,
                timestamp: Date.now(),
              };
              setMessages((prev) => [...prev, msg]);
              onMessage?.(fullText, false);
              mcBufferRef.current = "";
              
              // ç· ã‚ã®è¨€è‘‰ã‚’æ¤œå‡ºã—ãŸã‚‰è‡ªå‹•çµ‚äº†
              const endingPhrases = [
                "ãƒã‚¤ãƒã‚¤",
                "ã°ã„ã°ã„",
                "ã¾ãŸæ¬¡å›",
                "ã¾ãŸã­",
                "ãŠé€ã‚Šã—ã¾ã—ãŸ",
                "ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã—ãŸ",
                "ãã‚Œã§ã¯",
                "ã¾ãŸæ¥é€±",
                "ã•ã‚ˆã†ãªã‚‰",
              ];
              const isEnding = endingPhrases.some(phrase => fullText.includes(phrase));
              // ã€Œçµã€ãƒãƒ£ãƒ—ã‚¿ãƒ¼ï¼ˆ4ï¼‰ã§ç· ã‚ã®è¨€è‘‰ãŒå‡ºãŸã‚‰è‡ªå‹•çµ‚äº†
              if (isEnding && currentChapterRef.current >= 4) {
                console.log("ğŸ¬ ç•ªçµ„çµ‚äº†ã‚’æ¤œå‡º - è‡ªå‹•çµ‚äº†");
                setTimeout(() => {
                  onAutoEnd?.();
                }, 2000); // 2ç§’å¾…ã£ã¦ã‹ã‚‰çµ‚äº†ï¼ˆä½™éŸ»ã‚’æŒãŸã›ã‚‹ï¼‰
              }
              
              // MCã®ã‚¿ãƒ¼ãƒ³å®Œäº†å¾Œã«ãƒ‡ã‚£ãƒ¬ã‚¯ã‚¿ãƒ¼ãƒã‚§ãƒƒã‚¯
              messageCountRef.current++;
              if (messageCountRef.current - lastDirectorCheckRef.current >= DIRECTOR_CHECK_INTERVAL) {
                lastDirectorCheckRef.current = messageCountRef.current;
                checkDirector();
              }
            }
            
            if (audioQueueRef.current.length === 0 && !isPlayingRef.current) {
              setConversationState("listening");
            }
          }
          
          // ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç™ºè¨€ã‚’ãƒ•ãƒ©ãƒƒã‚·ãƒ¥ï¼ˆinputTranscriptionãŒè“„ç©ã•ã‚Œã¦ã„ã‚‹å ´åˆï¼‰
          // generationCompleteã®ã‚¿ã‚¤ãƒŸãƒ³ã‚°ã§ãƒ•ãƒ©ãƒƒã‚·ãƒ¥
          if (content.generationComplete) {
            if (userBufferRef.current.trim() && userBufferRef.current.trim().length >= 2) {
              const userText = userBufferRef.current.trim();
              console.log("ğŸ¤ ãƒ¦ãƒ¼ã‚¶ãƒ¼:", userText);
              const userMsg: Message = {
                role: "user",
                content: userText,
                timestamp: Date.now(),
              };
              setMessages((prev) => [...prev, userMsg]);
              onMessage?.(userText, true);
              userBufferRef.current = "";
              
              // ãƒ‡ã‚£ãƒ¬ã‚¯ã‚¿ãƒ¼ãƒã‚§ãƒƒã‚¯
              messageCountRef.current++;
              if (messageCountRef.current - lastDirectorCheckRef.current >= DIRECTOR_CHECK_INTERVAL) {
                lastDirectorCheckRef.current = messageCountRef.current;
                checkDirector();
              }
            }
          }

          // å‰²ã‚Šè¾¼ã¿æ¤œå‡º
          if (content.interrupted) {
            interruptPlayback();
          }
        }

        // ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç™ºè©±æ¤œå‡ºï¼ˆclientContentã‚¨ã‚³ãƒ¼ - ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
        if (data.clientContent?.turns) {
          for (const turn of data.clientContent.turns) {
            if (turn.role === "user" && turn.parts) {
              for (const part of turn.parts) {
                if (part.text) {
                  const msg: Message = {
                    role: "user",
                    content: part.text,
                    timestamp: Date.now(),
                  };
                  setMessages((prev) => [...prev, msg]);
                  onMessage?.(part.text, true);
                }
              }
            }
          }
        }
      };

      ws.onerror = (error) => {
        console.error("WebSocket error:", error);
        console.error("WebSocket readyState:", ws.readyState);
        setConnectionState("error");
        onError?.("WebSocketæ¥ç¶šã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ");
      };

      ws.onclose = (event) => {
        console.log("WebSocketåˆ‡æ–­:", event.code, event.reason);
        setConnectionState("disconnected");
        setConversationState("idle");

        // ç•°å¸¸åˆ‡æ–­ã®å ´åˆã¯å†æ¥ç¶šã‚’è©¦ã¿ã‚‹
        if (event.code !== 1000 && event.code !== 1001) {
          reconnectTimeoutRef.current = setTimeout(() => {
            console.log("å†æ¥ç¶šä¸­...");
            connect();
          }, 3000);
        }
      };
    } catch (error: any) {
      console.error("Connection error:", error);
      setConnectionState("error");
      onError?.(error.message || "æ¥ç¶šã«å¤±æ•—ã—ã¾ã—ãŸ");
    }
  }, [connectionState, mcId, theme, memo, isAudioEnabled, onMessage, onError, resetInactivityTimeout, decodeAudioData, playNextAudio, messagesRef]);

  // ãƒã‚¤ã‚¯å…¥åŠ›ç”¨ã®AudioContextï¼ˆåˆ¥ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼‰
  const micContextRef = useRef<AudioContext | null>(null);
  const workletNodeRef = useRef<AudioWorkletNode | null>(null);

  // ãƒ€ã‚¦ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°é–¢æ•°ï¼ˆ48kHz â†’ 16kHz ãªã©ï¼‰
  const downsample = useCallback((inputData: Float32Array, inputSampleRate: number, outputSampleRate: number): Int16Array => {
    const ratio = inputSampleRate / outputSampleRate;
    const outputLength = Math.floor(inputData.length / ratio);
    const output = new Int16Array(outputLength);
    
    for (let i = 0; i < outputLength; i++) {
      const inputIndex = Math.floor(i * ratio);
      const s = Math.max(-1, Math.min(1, inputData[inputIndex]));
      output[i] = s < 0 ? s * 0x8000 : s * 0x7fff;
    }
    
    return output;
  }, []);

  // ãƒã‚¤ã‚¯å…¥åŠ›ã®å‡¦ç†é–‹å§‹ï¼ˆAudioWorkletä½¿ç”¨ï¼‰
  const startAudioCapture = useCallback(async () => {
    if (!mediaStreamRef.current || !wsRef.current) {
      console.error("Cannot start audio capture - missing refs");
      return;
    }

    try {
      // ãƒã‚¤ã‚¯ç”¨ã«åˆ¥ã®AudioContextã‚’ä½œæˆ
      micContextRef.current = new AudioContext();
      const inputSampleRate = micContextRef.current.sampleRate;
      
      // AudioWorkletãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
      await micContextRef.current.audioWorklet.addModule('/audio-capture-processor.js');
      console.log("ğŸ¤ ãƒã‚¤ã‚¯æ¥ç¶šå®Œäº†");
      
      const source = micContextRef.current.createMediaStreamSource(mediaStreamRef.current);
      const workletNode = new AudioWorkletNode(micContextRef.current, 'audio-capture-processor');
      workletNodeRef.current = workletNode;
      
      let frameCount = 0;
      
      // AudioWorkletã‹ã‚‰ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å—ä¿¡
      workletNode.port.onmessage = (event) => {
        const data = event.data;
        
        // ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°ã¯ç„¡è¦–ï¼ˆå¿…è¦æ™‚ã®ã¿æœ‰åŠ¹åŒ–ï¼‰
        if (data.type === 'debug') {
          return;
        }
        
        if (data.type === 'audioData') {
          if (wsRef.current?.readyState !== WebSocket.OPEN) return;
          
          frameCount++;
          
          // Float32ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰RMSè¨ˆç®—
          const float32Data = new Float32Array(data.float32Data);
          const rmsVal = Math.sqrt(float32Data.reduce((sum: number, val: number) => sum + val * val, 0) / float32Data.length);
          
          // ãƒ€ã‚¦ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆinputSampleRate â†’ 16kHzï¼‰
          const int16Data = downsample(float32Data, inputSampleRate, 16000);
          
          // Base64ã«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
          const uint8 = new Uint8Array(int16Data.buffer);
          let binary = "";
          for (let i = 0; i < uint8.byteLength; i++) {
            binary += String.fromCharCode(uint8[i]);
          }
          const base64Audio = btoa(binary);
          
          // éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’é€ä¿¡
          const audioMessage = {
            realtimeInput: {
              mediaChunks: [
                {
                  mimeType: "audio/pcm;rate=16000",
                  data: base64Audio,
                },
              ],
            },
          };
          
          wsRef.current.send(JSON.stringify(audioMessage));
          
          // ç™ºè©±æ¤œå‡ºæ™‚ã«å‰²ã‚Šè¾¼ã¿ï¼ˆãƒ‡ãƒã‚¦ãƒ³ã‚¹ + ã‚¯ãƒ¼ãƒ«ãƒ€ã‚¦ãƒ³ä»˜ãï¼‰
          const now = Date.now();
          const timeSinceLastInterrupt = now - lastInterruptTimeRef.current;
          
          if (conversationStateRef.current === "speaking" && timeSinceLastInterrupt > INTERRUPT_COOLDOWN) {
            if (rmsVal > SPEECH_DETECTION_THRESHOLD) {
              speechDetectionCountRef.current++;
              // é€£ç¶šã—ã¦é–¾å€¤ã‚’è¶…ãˆãŸå ´åˆã®ã¿å‰²ã‚Šè¾¼ã¿
              if (speechDetectionCountRef.current >= SPEECH_DETECTION_FRAMES) {
                console.log("ğŸ”‡ ãƒ¦ãƒ¼ã‚¶ãƒ¼ç™ºè©±æ¤œå‡º - MCéŸ³å£°ã‚’ä¸­æ–­");
                interruptPlayback();
                setConversationState("listening");
                speechDetectionCountRef.current = 0;
                lastInterruptTimeRef.current = now;
              }
            } else {
              // é–¾å€¤ã‚’ä¸‹å›ã£ãŸã‚‰ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼ã‚’ãƒªã‚»ãƒƒãƒˆ
              speechDetectionCountRef.current = 0;
            }
          } else {
            speechDetectionCountRef.current = 0;
          }
        }
      };
      
      source.connect(workletNode);
      // WorkletNodeã‚’å‡ºåŠ›ã«æ¥ç¶šï¼ˆç„¡éŸ³ã‚’å‡ºåŠ›ï¼‰
      workletNode.connect(micContextRef.current.destination);
      
    } catch (error) {
      console.error("Failed to start AudioWorklet, falling back to ScriptProcessor:", error);
      // ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ScriptProcessorã‚’ä½¿ç”¨
      startAudioCaptureWithScriptProcessor();
    }
  }, [interruptPlayback, downsample]);

  // ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ã®ScriptProcessorï¼ˆå¤ã„ãƒ–ãƒ©ã‚¦ã‚¶ç”¨ï¼‰
  const startAudioCaptureWithScriptProcessor = useCallback(() => {
    if (!mediaStreamRef.current || !wsRef.current) return;
    
    console.log("ğŸ¤ ãƒã‚¤ã‚¯æ¥ç¶šå®Œäº† (ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯)");
    
    micContextRef.current = new AudioContext();
    const inputSampleRate = micContextRef.current.sampleRate;
    
    const source = micContextRef.current.createMediaStreamSource(mediaStreamRef.current);
    const processor = micContextRef.current.createScriptProcessor(4096, 1, 1);
    processorRef.current = processor;

    let frameCount = 0;
    processor.onaudioprocess = (event) => {
      if (wsRef.current?.readyState !== WebSocket.OPEN) return;

      const inputData = event.inputBuffer.getChannelData(0);
      frameCount++;
      
      // ãƒ€ã‚¦ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
      const int16Data = downsample(inputData, inputSampleRate, 16000);

      // Base64ã«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
      const uint8 = new Uint8Array(int16Data.buffer);
      let binary = "";
      for (let i = 0; i < uint8.byteLength; i++) {
        binary += String.fromCharCode(uint8[i]);
      }
      const base64Audio = btoa(binary);

      // éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’é€ä¿¡
      const audioMessage = {
        realtimeInput: {
          mediaChunks: [
            {
              mimeType: "audio/pcm;rate=16000",
              data: base64Audio,
            },
          ],
        },
      };

      wsRef.current.send(JSON.stringify(audioMessage));
    };

    source.connect(processor);
    processor.connect(micContextRef.current.destination);
  }, [downsample]);

  // åˆ‡æ–­
  const disconnect = useCallback(() => {
    if (reconnectTimeoutRef.current) {
      clearTimeout(reconnectTimeoutRef.current);
    }
    if (inactivityTimeoutRef.current) {
      clearTimeout(inactivityTimeoutRef.current);
    }

    // AudioWorkletNodeã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
    if (workletNodeRef.current) {
      workletNodeRef.current.disconnect();
      workletNodeRef.current = null;
    }

    if (processorRef.current) {
      processorRef.current.disconnect();
      processorRef.current = null;
    }

    // ãƒã‚¤ã‚¯ç”¨AudioContextã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
    if (micContextRef.current) {
      micContextRef.current.close();
      micContextRef.current = null;
    }

    if (mediaStreamRef.current) {
      mediaStreamRef.current.getTracks().forEach((track) => track.stop());
      mediaStreamRef.current = null;
    }

    if (audioContextRef.current) {
      audioContextRef.current.close();
      audioContextRef.current = null;
    }

    if (wsRef.current) {
      wsRef.current.close(1000, "User disconnected");
      wsRef.current = null;
    }

    audioQueueRef.current = [];
    isPlayingRef.current = false;
    setConnectionState("disconnected");
    setConversationState("idle");
  }, []);

  // ãƒ†ã‚­ã‚¹ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é€ä¿¡ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ï¼‰
  const sendTextMessage = useCallback((text: string) => {
    if (!wsRef.current || wsRef.current.readyState !== WebSocket.OPEN) return;

    const message = {
      clientContent: {
        turns: [
          {
            role: "user",
            parts: [{ text }],
          },
        ],
        turnComplete: true,
      },
    };

    wsRef.current.send(JSON.stringify(message));

    const msg: Message = {
      role: "user",
      content: text,
      timestamp: Date.now(),
    };
    setMessages((prev) => [...prev, msg]);
    onMessage?.(text, true);
    setConversationState("thinking");
  }, [onMessage]);

  // ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
  useEffect(() => {
    return () => {
      disconnect();
    };
  }, [disconnect]);

  return {
    connectionState,
    conversationState,
    messages,
    isAudioEnabled,
    setIsAudioEnabled,
    connect,
    disconnect,
    sendTextMessage,
    interruptPlayback,
  };
}

